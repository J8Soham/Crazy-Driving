{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip3 install highway_env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium\n",
    "import highway_env\n",
    "import numpy as np\n",
    "import pprint\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import copy\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "env = gymnasium.make(\"highway-v0\", render_mode='rgb_array')\n",
    "# pprint.pprint(env.unwrapped.config)\n",
    "env.unwrapped.config[\"lanes_count\"] = 3\n",
    "env.unwrapped.config[\"duration\"] = 10\n",
    "env.unwrapped.config[\"vehicles_density\"] = 2 # 3\n",
    "env.unwrapped.config[\"vehicles_count\"] = 10\n",
    "\n",
    "# env.unwrapped.config[\"action\"][\"type\"] = \"DiscreteAction\"\n",
    "# ACTION_SIZE = 9\n",
    "ACTION_SIZE = 5\n",
    "observation = {\n",
    "        \"type\": \"Kinematics\",\n",
    "        \"vehicles_count\": 5,\n",
    "        \"features\": [\"x\", \"y\", \"vx\", \"vy\", \"cos_h\"],\n",
    "        \"features_range\": {\n",
    "            \"x\": [-100, 100],\n",
    "            \"y\": [-100, 100],\n",
    "            \"vx\": [-20, 20],\n",
    "            \"vy\": [-20, 20]\n",
    "        },\n",
    "        \"absolute\": True,\n",
    "        # \"absolute\": True,\n",
    "        \"order\": \"sorted\"\n",
    "    }\n",
    "env.unwrapped.config[\"observation\"] = observation\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class Model(nn.Module):\n",
    "    def __init__(self, input_size, output_size, hidden_size=50):\n",
    "        super().__init__()\n",
    "        self.input_size = input_size\n",
    "        self.output_size = output_size\n",
    "        self.layer_1 = nn.Linear(input_size, hidden_size)\n",
    "        self.norm_1 = nn.BatchNorm1d(hidden_size)\n",
    "        self.layer_2 = nn.Linear(hidden_size, hidden_size)\n",
    "        self.norm_2 = nn.BatchNorm1d(hidden_size)\n",
    "        self.layer_3 = nn.Linear(hidden_size, output_size)\n",
    "        self.norm_3 = nn.BatchNorm1d(output_size)\n",
    "\n",
    "    def forward(self, obs, batch_size=1):\n",
    "        batch_norm_on = batch_size != 1\n",
    "        if obs is None:\n",
    "            retval = torch.tensor([[1/self.output_size] * self.output_size] * batch_size)\n",
    "            return retval\n",
    "        \n",
    "       \n",
    "        x = torch.tensor(obs)\n",
    "        x = x.view(-1, self.input_size)\n",
    "\n",
    "\n",
    "        x = self.layer_1(x)\n",
    "        if batch_norm_on:\n",
    "            x = self.norm_1(x)\n",
    "        x = F.sigmoid(x)\n",
    "        x = self.layer_2(x)\n",
    "        if batch_norm_on:\n",
    "            x = self.norm_2(x)\n",
    "        x = F.sigmoid(x)\n",
    "        x = self.layer_3(x)\n",
    "        if batch_norm_on:\n",
    "            x = self.norm_3(x)\n",
    "        x = F.softmax(x, dim=1)\n",
    "        \n",
    "        return x\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "\n",
    "class ReplayMemory(object):\n",
    "    def __init__(self, capacity=1000):\n",
    "        self.capacity = capacity\n",
    "        self.memory = []\n",
    "\n",
    "    def push(self, *args):\n",
    "        \"\"\"Save a transition\"\"\"\n",
    "        self.memory.append(args)\n",
    "        if len(self.memory) > self.capacity:\n",
    "            self.memory.pop(0)\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        batch = random.sample(self.memory, batch_size)\n",
    "        states = []\n",
    "        actions = []\n",
    "        rewards = []\n",
    "        for i, memory in enumerate(batch):\n",
    "            states.append(torch.tensor(memory[0]))\n",
    "            actions.append(torch.tensor(memory[1]))\n",
    "            rewards.append(torch.tensor(memory[2]))\n",
    "        states = torch.stack(states)\n",
    "        actions = torch.stack(actions)\n",
    "        rewards = torch.stack(rewards)\n",
    "            \n",
    "        return states, actions, rewards\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_action(obs, policy_net):\n",
    "    if obs is None:\n",
    "        return 0\n",
    "    policy = policy_net(obs)\n",
    "    # policy = policy.detach().numpy().flatten()\n",
    "    # return np.random.choice(len(policy), p=policy)\n",
    "    return torch.argmax(policy, dim=1).item()\n",
    "\n",
    "def optimize_model(policy_net, value_net, memory, BATCH_SIZE, GAMMA, optimizer):\n",
    "\n",
    "    if len(memory) < BATCH_SIZE:\n",
    "        return\n",
    "    # print('learn')\n",
    "    state_batch, action_batch, reward_batch = memory.sample(BATCH_SIZE)\n",
    "\n",
    "    # Compute Q(s_t, a) - the model computes Q(s_t), then we select the\n",
    "    # columns of actions taken. These are the actions which would've been taken\n",
    "    # for each batch state according to policy_net\n",
    "\n",
    "    state_action_values = policy_net(state_batch, batch_size=BATCH_SIZE).gather(1, action_batch.unsqueeze(1))\n",
    "    # Compute V(s_{t+1}) for all next states.\n",
    "    # Expected values of actions for non_final_next_states are computed based\n",
    "    # on the \"older\" target_net; selecting their best reward with max(1).values\n",
    "    # This is merged based on the mask, such that we'll have either the expected\n",
    "    # state value or 0 in case the state was final.\n",
    "\n",
    "    next_state_values = torch.max(value_net(state_batch, batch_size=BATCH_SIZE), dim=1).values\n",
    "    # Compute the expected Q values\n",
    "    expected_state_action_values = (next_state_values * GAMMA) + reward_batch\n",
    "\n",
    "    # Compute Huber loss\n",
    "    criterion = nn.SmoothL1Loss()\n",
    "    loss = criterion(state_action_values, expected_state_action_values.unsqueeze(1))\n",
    "    print(f'Loss: {loss:.2f}')\n",
    "    # [batch_size, 1, 1]\n",
    "\n",
    "    # Optimize the model\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    # In-place gradient clipping\n",
    "    torch.nn.utils.clip_grad_value_(policy_net.parameters(), 100)\n",
    "    optimizer.step()\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "OBS_SIZE = env.unwrapped.config[\"observation\"][\"vehicles_count\"] * len(env.unwrapped.config[\"observation\"][\"features\"])\n",
    "env.unwrapped.config[\"duration\"] = 20\n",
    "\n",
    "EPOCHS = 10000\n",
    "batch_size = 16\n",
    "GAMMA = 0.99\n",
    "TAU = 0.005\n",
    "LR = 1e-2\n",
    "\n",
    "policy_net = Model(OBS_SIZE, ACTION_SIZE)\n",
    "value_net = Model(OBS_SIZE, ACTION_SIZE)\n",
    "optimizer = torch.optim.AdamW(policy_net.parameters(), lr=LR, amsgrad=True)\n",
    "\n",
    "\n",
    "all_parameters = list(policy_net.parameters()) + list(value_net.parameters())\n",
    "optimizer_random = torch.optim.Adam(all_parameters, lr=0.1)\n",
    "\n",
    "env.reset()\n",
    "\n",
    "memory = ReplayMemory()\n",
    "for epoch in range(EPOCHS):\n",
    "    print(f'epoch: {epoch}')\n",
    "    env.reset()\n",
    "    obs = None\n",
    "    done = truncated = False\n",
    "    \n",
    "    total_reward = 0\n",
    "    while not (done or truncated):\n",
    "        action = select_action(obs, policy_net)\n",
    "        next_obs, reward, done, truncated, info = env.step(action)\n",
    "        print(info)\n",
    "\n",
    "        if info[\"rewards\"][\"on_road_reward\"] == 0:\n",
    "            pass# reward -= 10\n",
    "        # \"features\": [\"x\", \"y\", \"vx\", \"vy\", \"cos_h\"],\n",
    "        if done or truncated:\n",
    "            break\n",
    "        if obs is not None:\n",
    "            # print(f'sub: {abs(next_obs[0][4] - obs[0][4])}')\n",
    "            # reward -= abs(next_obs[0][4] - obs[0][4]) * 0.2\n",
    "            memory.push(obs, action, reward)\n",
    "        obs = next_obs\n",
    "        \n",
    "        total_reward += reward\n",
    "        # print(total_reward)\n",
    "        \n",
    "        if epoch < 5: # 5\n",
    "            target = torch.Tensor([[1 / ACTION_SIZE] * ACTION_SIZE])\n",
    "            policy = policy_net(obs)\n",
    "            values = value_net(obs)\n",
    "            criterion = torch.nn.CrossEntropyLoss()\n",
    "            loss_policy = criterion(policy, target)\n",
    "            \n",
    "            loss_values = criterion(policy, values)\n",
    "            loss = loss_policy + loss_values\n",
    "            optimizer_random.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            \n",
    "\n",
    "        else:\n",
    "            print(f'policy: {policy_net(obs)}')\n",
    "            print(f'value: {value_net(obs)}')\n",
    "            optimize_model(policy_net, value_net, memory, batch_size, GAMMA, optimizer)\n",
    "\n",
    "            value_net_state_dict = value_net.state_dict()\n",
    "            policy_net_state_dict = policy_net.state_dict()\n",
    "            for key in policy_net_state_dict:\n",
    "                value_net_state_dict[key] = value_net_state_dict[key]*TAU + value_net_state_dict[key]*(1-TAU)\n",
    "            value_net.load_state_dict(value_net_state_dict)\n",
    "            env.render()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "GENERATIONS = 1000\n",
    "SEEDS_PER_GEN = 10\n",
    "MODELS = 10\n",
    "OBS_SIZE = env.unwrapped.config[\"observation\"][\"vehicles_count\"] * len(env.unwrapped.config[\"observation\"][\"features\"])\n",
    "\n",
    "models = [Model(OBS_SIZE, ACTION_SIZE) for i in range(MODELS)]\n",
    "\n",
    "for generation in range(GENERATIONS):\n",
    "    env.unwrapped.config[\"duration\"] = 25 + 0.5 * generation\n",
    "    print(f'Generation: {generation}')\n",
    "\n",
    "    # create models\n",
    "    scores = [0 for i in range(MODELS)]\n",
    "\n",
    "    for i in range(SEEDS_PER_GEN):\n",
    "        seed = np.random.randint(1_000_000)\n",
    "        for model_num in range(MODELS):\n",
    "            print(f'{model_num} ', end='')\n",
    "            render = False\n",
    "            if model_num % 1 == 0:\n",
    "                render = True\n",
    "            model = models[model_num]\n",
    "\n",
    "            env.reset(seed=seed)\n",
    "            done = truncated = False\n",
    "            obs = None\n",
    "            score = 0\n",
    "            while not (done or truncated):\n",
    "                probabilities = model(obs).view(-1).detach().numpy()\n",
    "                action = np.argmax(probabilities)\n",
    "                # action = np.random.choice(ACTION_SIZE, p=probabilities)\n",
    "                obs, reward, done, truncated, info = env.step(action)\n",
    "                if info[\"rewards\"][\"on_road_reward\"] == 0:\n",
    "                    # reward -= 10\n",
    "                    done = True\n",
    "                if render:\n",
    "                    env.render()\n",
    "                score += reward\n",
    "            print(score)\n",
    "\n",
    "            # if done:\n",
    "            #     score -= 10\n",
    "\n",
    "            scores[model_num] += score\n",
    "        print()\n",
    "    best_score = max(scores)\n",
    "    best_model_num = scores.index(best_score)\n",
    "    best_model = models[best_model_num]\n",
    "    print(f'Best model_num: {best_model_num} | score: {best_score}\\n')\n",
    "    models = [copy.deepcopy(best_model) for i in range(MODELS)]\n",
    "    change = 0.1 * (0.999 ** generation)\n",
    "    for model in models:\n",
    "        for param in model.parameters():\n",
    "            param.data += change * torch.randn_like(param)\n",
    "    torch.save(best_model.state_dict(), f'model_gen{generation}')\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
